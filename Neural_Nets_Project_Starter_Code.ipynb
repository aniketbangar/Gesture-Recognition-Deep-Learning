{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_doc = np.random.permutation(open('/notebooks/storage/Final_data/Collated_training/train.csv').readlines())\n",
    "# val_doc = np.random.permutation(open('/notebooks/storage/Final_data/Collated_training/val.csv').readlines())\n",
    "batch_size = 4\n",
    "\n",
    "train_doc = np.random.permutation(open('/home/asr563up/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/home/asr563up/Project_data/val.csv').readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy.\n",
    "\n",
    "Following new parameters were added to generator function to experiment with more parameters\n",
    "1. ablation -> experiment with smaller dataset and try to overfit on it \n",
    "2. batch_size -> an ideal batch_size can help us train the network better and faster\n",
    "3. image_frames_to_retain -> number of video frames to consider\n",
    "4. image_shape -> defaults to (100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size=batch_size, \n",
    "              ablation=-1, image_frames_to_retain=9, \n",
    "              image_shape=(100, 100)):\n",
    "    \n",
    "    img_idx = list(range(1, min(image_frames_to_retain+1, 31)))\n",
    "    x = len(img_idx)\n",
    "    width = image_shape[0]\n",
    "    height = image_shape[1]\n",
    "    \n",
    "    # code for ablation\n",
    "    if ablation != -1:\n",
    "        \n",
    "        folder_list_temp = [] \n",
    "        \n",
    "        labels = [0, 1, 2, 3, 4]\n",
    "        \n",
    "        for label in labels:\n",
    "            \n",
    "            folder_list_label = []\n",
    "            \n",
    "            searchText = re.compile('.*'+ str(label) + '\\\\n')\n",
    "            \n",
    "            folder_list_label = list(filter(searchText.match, folder_list))\n",
    "            \n",
    "            folder_list_label = folder_list_label[0: min(ablation, len(folder_list_label))]\n",
    "            \n",
    "            folder_list_temp = folder_list_temp + folder_list_label\n",
    "            \n",
    "        folder_list = folder_list_temp\n",
    " \n",
    "        \n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        \n",
    "        \n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(folder_list)/batch_size)\n",
    "        \n",
    "#         for batch in range(0):\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size, x, height, width, 3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    if image.shape[1]/image.shape[0] == 1:\n",
    "                        \n",
    "                        # image   has been brought to 224 * 224 res\n",
    "                        image = imresize(image, (height, width))\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        image = image[:, 20:140, :]\n",
    "                        image = imresize(image, (height, width))\n",
    "                        \n",
    "                    \n",
    "#                     img_reshaped_quantile = np.quantile(image.reshape(-1, 3), [0.05, 0.95], axis=1)\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = image[:, :, 0]/255\n",
    "                    batch_data[folder,idx,:,:,1] = image[:, :, 1]/255\n",
    "                    batch_data[folder,idx,:,:,2] = image[:, :, 2]/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        remaining_data_points = len(folder_list) - num_batches * batch_size\n",
    "        \n",
    "        # rerunning the generator for new epoch\n",
    "        if remaining_data_points == 0:\n",
    "            continue\n",
    "        \n",
    "        batch_data = np.zeros((remaining_data_points, x, height, width, 3))\n",
    "        batch_labels = np.zeros((remaining_data_points,5))\n",
    "        \n",
    "        for folder in range(num_batches * batch_size, len(folder_list)):\n",
    "            imgs = os.listdir(source_path+'/'+ t[folder].split(';')[0]) # read all the images in the folder\n",
    "            for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                image = imread(source_path+'/'+ t[folder].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                if image.shape[1]/image.shape[0] == 1:\n",
    "                        \n",
    "                    # image   has been brought to 224 * 224 res\n",
    "                    image = imresize(image, (height, width))\n",
    "                        \n",
    "                else:\n",
    "                        \n",
    "                    image = image[:, 20:140, :]\n",
    "                    image = imresize(image, (height, width))\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                    \n",
    "                batch_data[folder - num_batches * batch_size ,idx,:,:,0] = image[:, :, 0]/255\n",
    "                batch_data[folder - num_batches * batch_size,idx,:,:,1] = image[:, :, 1]/255\n",
    "                batch_data[folder- num_batches * batch_size,idx,:,:,2] = image[:, :, 2]/255\n",
    "                \n",
    "            batch_labels[folder - num_batches * batch_size, int(t[folder].strip().split(';')[2])] = 1\n",
    "        \n",
    "        yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 2\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "# train_path = '/notebooks/storage/Final_data/Collated_training/train'\n",
    "# val_path = '/notebooks/storage/Final_data/Collated_training/val'\n",
    "train_path = '/home/asr563up/Project_data/train'\n",
    "val_path = '/home/asr563up/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 2\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, GlobalMaxPool2D\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.metrics import AUC, Precision, Recall\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "#write your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeDistributed Conv2D + RNN(GRU) Modelm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 inspired network , we will be adding breaks in between to limit network at specific length . \n",
    "def extract_layers_from_vgg16(layers=5, image_frames_to_retain=9, image_shape=(100, 100), **kwargs):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "            input_shape=(image_frames_to_retain, image_shape[1], image_shape[0], 3)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "        MaxPooling2D(pool_size=(2,2),strides=(2,2))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            BatchNormalization()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if layers == 1:\n",
    "        return model\n",
    "\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "        MaxPooling2D(pool_size=(2,2),strides=(2,2))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            BatchNormalization()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if layers == 2:\n",
    "        return model\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "        MaxPooling2D(pool_size=(2,2),strides=(2,2))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            BatchNormalization()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if layers == 3:\n",
    "        return model\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "        MaxPooling2D(pool_size=(2,2),strides=(2,2))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if layers == 4:\n",
    "        return model\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "            Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        TimeDistributed(\n",
    "        MaxPooling2D(pool_size=(2,2),strides=(2,2))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if layers == 5:\n",
    "        return model\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def conv2d_rnn(layers=13, after_flatten_layer_size=3, after_flatten_layer_jump=2, \n",
    "               image_frames_to_retain=9, image_shape=(100, 100), **kwargs):\n",
    "    \n",
    "    model = extract_layers_from_vgg16(layers=layers, image_frames_to_retain=image_frames_to_retain,\n",
    "                                      image_shape=image_shape)\n",
    "    \n",
    "    model.add(\n",
    "    TimeDistributed(\n",
    "        Flatten())\n",
    "    )\n",
    "\n",
    "    \n",
    "    model.add(\n",
    "    GRU(30)\n",
    "    )\n",
    "    \n",
    "    for layer in range(after_flatten_layer_size, 1, -1):\n",
    "        \n",
    "        unit_size = 5 * ((after_flatten_layer_jump) ** (layer-1))\n",
    "        model.add(Dense(unit_size, activation='relu'))\n",
    "        \n",
    "    \n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv3d Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3D_model_core_layers(layers=5, initial_filter_size=32, \n",
    "                             filter_jump=2, jump_mode=\"*\",\n",
    "                            image_frames_to_retain=9, image_shape=(100, 100)):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    filter_size = initial_filter_size\n",
    "    \n",
    "    for layer in range(0, layers):\n",
    "        \n",
    "        model.add(\n",
    "            Conv3D(\n",
    "                filters=filter_size,\n",
    "                kernel_size=(2, 2, 2),\n",
    "                padding=\"same\",\n",
    "                activation=\"relu\",\n",
    "                input_shape=(image_frames_to_retain, image_shape[1], image_shape[0], 3)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "        if jump_mode == '*':\n",
    "            \n",
    "        \n",
    "            filter_size = filter_size * 2\n",
    "            \n",
    "    return model\n",
    "        \n",
    "        \n",
    "\n",
    "def conv3d_model(layers=3, \n",
    "                 initial_filter_size=32, \n",
    "                 filter_jump=2, \n",
    "                 jump_mode=\"*\", \n",
    "                 after_flatten_layer_size=3,\n",
    "                 after_flatten_layer_jump=2, image_frames_to_retain=9, image_shape=(100, 100)):\n",
    "    \n",
    "    model = conv3D_model_core_layers(layers=layers, initial_filter_size=initial_filter_size, \n",
    "                                     filter_jump=filter_jump, jump_mode=jump_mode,\n",
    "                                     image_frames_to_retain=image_frames_to_retain, image_shape=image_shape\n",
    "                                    )\n",
    "    \n",
    "    model.add(\n",
    "        Flatten()\n",
    "    )\n",
    "    \n",
    "    for layer in range(after_flatten_layer_size, 1, -1):\n",
    "        \n",
    "        unit_size = 5 * ((after_flatten_layer_jump) ** (layer-1))\n",
    "        model.add(Dense(unit_size, activation='relu'))\n",
    "        \n",
    "    \n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = conv2d_rnn(layers=2)\n",
    "# optimiser = optimizers.Adam()\n",
    "# model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy', AUC()])\n",
    "# print (model.summary())\n",
    "\n",
    "global_storage = []\n",
    "\n",
    "class ComputeMetrics(Callback):\n",
    "    \n",
    "    def __init__(self, storage, model_name, ablation, layer, \n",
    "                 initial_filter_size, filter_jump, \n",
    "                 jump_mode, after_flatten_layer_size, after_flatten_layer_jump, \n",
    "                 image_frames_to_retain, image_shape,\n",
    "                 *args, **kwargs):\n",
    "        \n",
    "        self.storage = storage\n",
    "        self.storage.append({})\n",
    "        self.storage[-1]['model_name'] = model_name\n",
    "        self.storage[-1]['ablation'] = ablation\n",
    "        self.storage[-1]['layer'] = layer\n",
    "        self.storage[-1]['initial_filter_size'] = initial_filter_size\n",
    "        self.storage[-1]['filter_jump'] = filter_jump\n",
    "        self.storage[-1]['jump_mode'] = jump_mode\n",
    "        self.storage[-1]['after_flatten_layer_size'] = after_flatten_layer_size\n",
    "        self.storage[-1]['after_flatten_layer_jump'] = after_flatten_layer_jump\n",
    "        self.storage[-1]['image_frames_to_retain'] = image_frames_to_retain\n",
    "        self.storage[-1]['image_shape'] = image_shape\n",
    "        \n",
    "        \n",
    "        super().__init__(*args, **kwargs);\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        temp_storage = logs\n",
    "        temp_storage['epoch'] = epoch\n",
    "        self.storage[-1].update(temp_storage)\n",
    "\n",
    "        \n",
    "def test_model(model, \n",
    "               layers=[1, 2, 3, 4, 5, 6], \n",
    "                ablations=[-1], \n",
    "                initial_filter_sizes=[32, 42], \n",
    "                filter_jumps=[2, 3], \n",
    "                after_flatten_layer_sizes=[3, 4],\n",
    "                after_flatten_layer_jumps=[2, 3], \n",
    "                image_frames_to_retain=[9], \n",
    "                image_shapes=[(100,100)]\n",
    "                ):\n",
    "\n",
    "    \n",
    "    for ablation in ablations:\n",
    "        if (num_train_sequences%batch_size) == 0 and ablation == -1:\n",
    "            steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "        elif ablation == -1:\n",
    "            steps_per_epoch = (num_train_sequences//batch_size) + 1 \n",
    "        elif ((ablation * 5)%batch_size) == 0 and ablation != -1:\n",
    "            steps_per_epoch = int(ablation * 5/batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = int(ablation * 5//batch_size)\n",
    "    \n",
    "\n",
    "        if (num_val_sequences%batch_size) == 0 and ablation == -1:\n",
    "            validation_steps = int(num_val_sequences/batch_size)\n",
    "        elif ablation == -1:\n",
    "            validation_steps = (num_val_sequences//batch_size) + 1\n",
    "        elif ((ablation * 5)%batch_size) == 0 and ablation != -1:\n",
    "            validation_steps = int(ablation * 5/batch_size)\n",
    "        else:\n",
    "            validation_steps = int(ablation * 5//batch_size)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        for layer in layers:\n",
    "            \n",
    "            for filter_size in initial_filter_sizes:\n",
    "                \n",
    "                for filter_jump in filter_jumps:\n",
    "                    \n",
    "                    for after_flatten_layer_size in after_flatten_layer_sizes:\n",
    "                        \n",
    "                        for after_flatten_layer_jump in after_flatten_layer_jumps:\n",
    "                            \n",
    "                            for image_frames_to_retain_val in image_frames_to_retain:                                \n",
    "                                \n",
    "                                for image_shape in image_shapes:\n",
    "                                    \n",
    "                                    \n",
    "                                    train_generator = generator(train_path, train_doc, batch_size, \n",
    "                                                                ablation=ablation, image_frames_to_retain=image_frames_to_retain_val, \n",
    "                                                                image_shape=image_shape)\n",
    "                                    val_generator = generator(val_path, val_doc, batch_size, \n",
    "                                                              ablation=ablation, image_frames_to_retain=image_frames_to_retain_val, \n",
    "                                                                image_shape=image_shape )\n",
    "                                    \n",
    "                                    \n",
    "                                    \n",
    "                                    print(layer,\n",
    "                                        filter_size, \n",
    "                                        filter_jump,\n",
    "                                        after_flatten_layer_size,\n",
    "                                        after_flatten_layer_jump,\n",
    "                                        image_frames_to_retain_val, image_shape)\n",
    "\n",
    "                                    try:\n",
    "                                        __model__ = model(\n",
    "                                            layers=layer,\n",
    "                                            initial_filter_size=filter_size, \n",
    "                                            filter_jump=filter_jump, \n",
    "                                            jump_mode=\"*\", \n",
    "                                            after_flatten_layer_size=after_flatten_layer_size,\n",
    "                                            after_flatten_layer_jump=after_flatten_layer_jump,\n",
    "                                            image_frames_to_retain=image_frames_to_retain_val, image_shape=image_shape\n",
    "                                            )\n",
    "                                    except Exception as err:\n",
    "                                        print(err)\n",
    "                                        # model cannot be created\n",
    "                                        continue\n",
    "                                        \n",
    "                                    \n",
    "\n",
    "                                    __model__.compile(optimizer=optimizers.SGD(), loss='categorical_crossentropy', \n",
    "                                      metrics=['categorical_accuracy', AUC()])\n",
    "                                    \n",
    "#                                     print(__model__.summary())\n",
    "                                    \n",
    "#                                     continue\n",
    "                                \n",
    "                                    \n",
    "\n",
    "    #             print(ablation)\n",
    "                                    __model__.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                                                 class_weight=None, workers=1, initial_epoch=0, \n",
    "                                                validation_data=val_generator, \n",
    "                                                validation_steps=validation_steps,\n",
    "                                                use_multiprocessing=False, callbacks=[ComputeMetrics(\n",
    "                                                    storage=global_storage,\n",
    "                                                    model_name='temp',\n",
    "                                                    ablation=ablation,\n",
    "                                                    layer=layer, \n",
    "                                                    initial_filter_size=filter_size, \n",
    "                                                    filter_jump=filter_jump, \n",
    "                                                    jump_mode=\"*\", \n",
    "                                                    after_flatten_layer_size=after_flatten_layer_size,\n",
    "                                                    after_flatten_layer_jump=after_flatten_layer_jump,\n",
    "                                                    image_frames_to_retain=image_frames_to_retain_val, image_shape=image_shape\n",
    "                                                )])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Analysis on various hyperparameters for Conv2d + GRU model\n",
    "\n",
    "Following are the hyperparameters of our learning alogrithm (it;s a function that helps us guess the ideal layer and parameter qunatity)\n",
    "1. layers -> [3, 4]\n",
    "4. after_flatten_layer_sizes=[2, 3],\n",
    "5. after_flatten_layer_jumps=[2, 3],\n",
    "6. image_frames_to_retain=[16],\n",
    "7. image_shapes=[(40, 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "global_storage = []\n",
    "test_model(conv2d_rnn, layers=[3, 4],  after_flatten_layer_sizes=[2, 3],\n",
    "           after_flatten_layer_jumps=[2, 3],\n",
    "           image_frames_to_retain=[16], image_shapes=[(40, 40)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filter_jump</th>\n",
       "      <th>layer</th>\n",
       "      <th>val_categorical_accuracy</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "      <th>auc</th>\n",
       "      <th>val_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.423831</td>\n",
       "      <td>0.767191</td>\n",
       "      <td>0.765575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.437406</td>\n",
       "      <td>0.768357</td>\n",
       "      <td>0.629250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.398190</td>\n",
       "      <td>0.724709</td>\n",
       "      <td>0.624475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.467572</td>\n",
       "      <td>0.788141</td>\n",
       "      <td>0.615713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.420814</td>\n",
       "      <td>0.769678</td>\n",
       "      <td>0.593825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.458522</td>\n",
       "      <td>0.775966</td>\n",
       "      <td>0.575350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.432881</td>\n",
       "      <td>0.771163</td>\n",
       "      <td>0.557413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.413273</td>\n",
       "      <td>0.743951</td>\n",
       "      <td>0.557087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.447964</td>\n",
       "      <td>0.755079</td>\n",
       "      <td>0.547963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.413273</td>\n",
       "      <td>0.770630</td>\n",
       "      <td>0.534975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.319759</td>\n",
       "      <td>0.640908</td>\n",
       "      <td>0.528763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.422323</td>\n",
       "      <td>0.766191</td>\n",
       "      <td>0.521425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.449472</td>\n",
       "      <td>0.754570</td>\n",
       "      <td>0.518975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    filter_jump  layer  val_categorical_accuracy  categorical_accuracy  \\\n",
       "5             3      3                      0.46              0.423831   \n",
       "7             3      3                      0.32              0.437406   \n",
       "0             2      3                      0.26              0.398190   \n",
       "11            2      3                      0.35              0.467572   \n",
       "9             2      3                      0.27              0.420814   \n",
       "4             3      3                      0.24              0.458522   \n",
       "8             2      3                      0.27              0.432881   \n",
       "1             2      3                      0.21              0.413273   \n",
       "10            2      3                      0.25              0.447964   \n",
       "6             3      3                      0.21              0.413273   \n",
       "12            3      3                      0.20              0.319759   \n",
       "2             2      3                      0.17              0.422323   \n",
       "3             2      3                      0.25              0.449472   \n",
       "\n",
       "         auc   val_auc  \n",
       "5   0.767191  0.765575  \n",
       "7   0.768357  0.629250  \n",
       "0   0.724709  0.624475  \n",
       "11  0.788141  0.615713  \n",
       "9   0.769678  0.593825  \n",
       "4   0.775966  0.575350  \n",
       "8   0.771163  0.557413  \n",
       "1   0.743951  0.557087  \n",
       "10  0.755079  0.547963  \n",
       "6   0.770630  0.534975  \n",
       "12  0.640908  0.528763  \n",
       "2   0.766191  0.521425  \n",
       "3   0.754570  0.518975  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sns.lineplot(data=df, x='layers', y='categorical_accuracy', hue='name')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# df = pd.DataFrame.from_dict(global_storage).sort_values('val_categorical_accuracy', axis=0, ascending=False)\n",
    "# df.to_csv('file1.csv')\n",
    "result = []\n",
    "\n",
    "for res in global_storage:\n",
    "    keys = list(res.keys())\n",
    "    search_auc_reg = re.compile('^auc_.*$')            \n",
    "    search_auc_key = list(filter(search_auc_reg.match, keys))\n",
    "    \n",
    "    search_auc_val_reg = re.compile('^.+auc_.*$')            \n",
    "    search_auc_val_key = list(filter(search_auc_val_reg.match, keys))\n",
    "    res['auc'] = res[search_auc_key[0]]\n",
    "    res['val_auc'] = res[search_auc_val_key[0]]\n",
    "    \n",
    "\n",
    "    \n",
    "df = pd.DataFrame.from_dict(global_storage).sort_values('val_auc', axis=0, ascending=False)\n",
    "\n",
    "\n",
    "df_all_layer = df.loc[:,['filter_jump',\n",
    "                                        'layer', 'val_categorical_accuracy',\n",
    "                                       'categorical_accuracy', 'auc', 'val_auc']]\n",
    "display(df_all_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Analysis on various hyperparameters for Conv3d model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the hyperparameters of our learning alogrithm (it;s a function that helps us guess the ideal layer and parameter qunatity)\n",
    "1. layers -> [3, 4]\n",
    "2. initial_filter_sizes -> [30, 60]\n",
    "3. filter_jumps -> [2, 3]\n",
    "4. after_flatten_layer_sizes=[3, 4],\n",
    "5. after_flatten_layer_jumps=[2, 3],\n",
    "6. image_frames_to_retain=[16],\n",
    "7. image_shapes=[(40, 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 30 2 3 2 16 (40, 40)\n",
      "Epoch 1/2\n",
      "166/166 [==============================] - 119s 718ms/step - loss: 1.7095 - categorical_accuracy: 0.2428 - auc_130: 0.5478 - val_loss: 2.3375 - val_categorical_accuracy: 0.2000 - val_auc_130: 0.5298\n",
      "Epoch 2/2\n",
      "166/166 [==============================] - 115s 693ms/step - loss: 1.5009 - categorical_accuracy: 0.3454 - auc_130: 0.6535 - val_loss: 2.2010 - val_categorical_accuracy: 0.2900 - val_auc_130: 0.6153\n",
      "3 30 3 3 2 16 (40, 40)\n",
      "Epoch 1/2\n",
      "166/166 [==============================] - 120s 724ms/step - loss: 1.7398 - categorical_accuracy: 0.1931 - auc_131: 0.5059 - val_loss: 0.9105 - val_categorical_accuracy: 0.2400 - val_auc_131: 0.5566\n",
      "Epoch 2/2\n",
      "166/166 [==============================] - 115s 694ms/step - loss: 1.6062 - categorical_accuracy: 0.1840 - auc_131: 0.5093 - val_loss: 1.5339 - val_categorical_accuracy: 0.1700 - val_auc_131: 0.5638\n",
      "3 60 2 3 2 16 (40, 40)\n",
      "Epoch 1/2\n",
      "166/166 [==============================] - 212s 1s/step - loss: 1.6596 - categorical_accuracy: 0.1765 - auc_132: 0.4763 - val_loss: 7.4795 - val_categorical_accuracy: 0.2100 - val_auc_132: 0.5079\n",
      "Epoch 2/2\n",
      "166/166 [==============================] - 207s 1s/step - loss: 1.6105 - categorical_accuracy: 0.1961 - auc_132: 0.4941 - val_loss: 1.5329 - val_categorical_accuracy: 0.2500 - val_auc_132: 0.6234\n",
      "3 60 3 3 2 16 (40, 40)\n",
      "Epoch 1/2\n",
      "166/166 [==============================] - 212s 1s/step - loss: 1.6123 - categorical_accuracy: 0.1900 - auc_133: 0.4809 - val_loss: 1.9646 - val_categorical_accuracy: 0.1800 - val_auc_133: 0.4995\n",
      "Epoch 2/2\n",
      "166/166 [==============================] - 206s 1s/step - loss: 1.6099 - categorical_accuracy: 0.1795 - auc_133: 0.5010 - val_loss: 1.6356 - val_categorical_accuracy: 0.2300 - val_auc_133: 0.5281\n",
      "4 30 2 3 2 16 (40, 40)\n",
      "Epoch 1/2\n",
      "166/166 [==============================] - 122s 734ms/step - loss: 1.6320 - categorical_accuracy: 0.2323 - auc_134: 0.5596 - val_loss: 1.5409 - val_categorical_accuracy: 0.1700 - val_auc_134: 0.5235\n",
      "Epoch 2/2\n",
      "166/166 [==============================] - 117s 703ms/step - loss: 1.5594 - categorical_accuracy: 0.3092 - auc_134: 0.6492 - val_loss: 1.2892 - val_categorical_accuracy: 0.2800 - val_auc_134: 0.5992\n",
      "4 30 3 3 2 16 (40, 40)\n",
      "Epoch 1/2\n",
      "166/166 [==============================] - 121s 730ms/step - loss: 1.5770 - categorical_accuracy: 0.2941 - auc_135: 0.6318 - val_loss: 3.5814 - val_categorical_accuracy: 0.1600 - val_auc_135: 0.5135\n",
      "Epoch 2/2\n",
      "166/166 [==============================] - 117s 704ms/step - loss: 1.3607 - categorical_accuracy: 0.3861 - auc_135: 0.7471 - val_loss: 2.7579 - val_categorical_accuracy: 0.2200 - val_auc_135: 0.5657\n",
      "4 60 2 3 2 16 (40, 40)\n",
      "Epoch 1/2\n",
      "166/166 [==============================] - 217s 1s/step - loss: 1.5673 - categorical_accuracy: 0.2956 - auc_136: 0.6433 - val_loss: 1.9107 - val_categorical_accuracy: 0.1600 - val_auc_136: 0.5321\n",
      "Epoch 2/2\n",
      "166/166 [==============================] - 211s 1s/step - loss: 1.3243 - categorical_accuracy: 0.3876 - auc_136: 0.7560 - val_loss: 2.4130 - val_categorical_accuracy: 0.2400 - val_auc_136: 0.5836\n",
      "4 60 3 3 2 16 (40, 40)\n",
      "Epoch 1/2\n",
      "166/166 [==============================] - 216s 1s/step - loss: 1.6923 - categorical_accuracy: 0.1855 - auc_137: 0.4946 - val_loss: 1.4893 - val_categorical_accuracy: 0.2100 - val_auc_137: 0.5186\n",
      "Epoch 2/2\n",
      "166/166 [==============================] - 211s 1s/step - loss: 1.5819 - categorical_accuracy: 0.2624 - auc_137: 0.5673 - val_loss: 2.1233 - val_categorical_accuracy: 0.1900 - val_auc_137: 0.5154\n"
     ]
    }
   ],
   "source": [
    "# test_model(conv3d_model, layers=[4], \n",
    "#            initial_filter_sizes=[30], \n",
    "#            filter_jumps=[2], after_flatten_layer_sizes=[3],\n",
    "#            after_flatten_layer_jumps=[2],\n",
    "#            image_frames_to_retain=[16], image_shapes=[(40, 40)])\n",
    "global_storage = []\n",
    "test_model(conv3d_model, layers=[3, 4], \n",
    "           initial_filter_sizes=[30, 60], \n",
    "           filter_jumps=[2, 3], after_flatten_layer_sizes=[3],\n",
    "           after_flatten_layer_jumps=[2],\n",
    "           image_frames_to_retain=[16], image_shapes=[(40, 40)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial_filter_size</th>\n",
       "      <th>filter_jump</th>\n",
       "      <th>layer</th>\n",
       "      <th>val_categorical_accuracy</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "      <th>auc</th>\n",
       "      <th>val_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.494113</td>\n",
       "      <td>0.623388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.653538</td>\n",
       "      <td>0.615312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.309201</td>\n",
       "      <td>0.649221</td>\n",
       "      <td>0.599238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.387632</td>\n",
       "      <td>0.755966</td>\n",
       "      <td>0.583625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.386124</td>\n",
       "      <td>0.747090</td>\n",
       "      <td>0.565687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.184012</td>\n",
       "      <td>0.509326</td>\n",
       "      <td>0.563800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.500982</td>\n",
       "      <td>0.528100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.206637</td>\n",
       "      <td>0.494496</td>\n",
       "      <td>0.518750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.262443</td>\n",
       "      <td>0.567326</td>\n",
       "      <td>0.515438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   initial_filter_size  filter_jump  layer  val_categorical_accuracy  \\\n",
       "3                   60            2      3                      0.25   \n",
       "1                   30            2      3                      0.29   \n",
       "5                   30            2      4                      0.28   \n",
       "7                   60            2      4                      0.24   \n",
       "6                   30            3      4                      0.22   \n",
       "2                   30            3      3                      0.17   \n",
       "4                   60            3      3                      0.23   \n",
       "0                   30            2      3                      0.22   \n",
       "8                   60            3      4                      0.19   \n",
       "\n",
       "   categorical_accuracy       auc   val_auc  \n",
       "3              0.196078  0.494113  0.623388  \n",
       "1              0.345400  0.653538  0.615312  \n",
       "5              0.309201  0.649221  0.599238  \n",
       "7              0.387632  0.755966  0.583625  \n",
       "6              0.386124  0.747090  0.565687  \n",
       "2              0.184012  0.509326  0.563800  \n",
       "4              0.179487  0.500982  0.528100  \n",
       "0              0.206637  0.494496  0.518750  \n",
       "8              0.262443  0.567326  0.515438  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sns.lineplot(data=pd.from_dict())\n",
    "# df = pd.DataFrame.from_dict(global_storage)\n",
    "# sns.lineplot(data=df, x='layers', y='categorical_accuracy', hue='name')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# df = pd.DataFrame.from_dict(global_storage).sort_values('val_categorical_accuracy', axis=0, ascending=False)\n",
    "# df.to_csv('file1.csv')\n",
    "result = []\n",
    "\n",
    "for res in global_storage:\n",
    "    keys = list(res.keys())\n",
    "    search_auc_reg = re.compile('^auc_.*$')            \n",
    "    search_auc_key = list(filter(search_auc_reg.match, keys))\n",
    "    \n",
    "    search_auc_val_reg = re.compile('^.+auc_.*$')            \n",
    "    search_auc_val_key = list(filter(search_auc_val_reg.match, keys))\n",
    "    res['auc'] = res[search_auc_key[0]]\n",
    "    res['val_auc'] = res[search_auc_val_key[0]]\n",
    "    \n",
    "\n",
    "    \n",
    "df = pd.DataFrame.from_dict(global_storage).sort_values('val_auc', axis=0, ascending=False)\n",
    "\n",
    "\n",
    "df_all_layer = df.loc[:,['initial_filter_size', 'filter_jump',\n",
    "                                        'layer', 'val_categorical_accuracy',\n",
    "                                       'categorical_accuracy', 'auc', 'val_auc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above table shows that model 1 with `30 initial filter` size , `3 conv3d layers` outperforms other model in terms of `val_auc` and `val_categorcial_accuracy`\n",
    "\n",
    "Taking model 1 as base model , we will try to tune it further for higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ideation 1: Iterating over final dense layers , image size and total number of video frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 30 2 2 2 16 (40, 40)\n",
      "3 30 2 2 2 16 (60, 60)\n",
      "3 30 2 2 2 20 (40, 40)\n",
      "3 30 2 2 2 20 (60, 60)\n",
      "3 30 2 2 3 16 (40, 40)\n",
      "3 30 2 2 3 16 (60, 60)\n",
      "3 30 2 2 3 20 (40, 40)\n",
      "3 30 2 2 3 20 (60, 60)\n",
      "3 30 2 3 2 16 (40, 40)\n",
      "3 30 2 3 2 16 (60, 60)\n",
      "3 30 2 3 2 20 (40, 40)\n",
      "3 30 2 3 2 20 (60, 60)\n",
      "3 30 2 3 3 16 (40, 40)\n",
      "3 30 2 3 3 16 (60, 60)\n",
      "3 30 2 3 3 20 (40, 40)\n",
      "3 30 2 3 3 20 (60, 60)\n"
     ]
    }
   ],
   "source": [
    "global_storage = []\n",
    "num_epochs = 3\n",
    "test_model(conv3d_model, layers=[3], \n",
    "           initial_filter_sizes=[30], \n",
    "           filter_jumps=[2], after_flatten_layer_sizes=[2, 3],\n",
    "           after_flatten_layer_jumps=[2, 3],\n",
    "           image_frames_to_retain=[16, 20], image_shapes=[(40, 40), (60, 60)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>after_flatten_layer_size</th>\n",
       "      <th>after_flatten_layer_jump</th>\n",
       "      <th>image_frames_to_retain</th>\n",
       "      <th>image_shape</th>\n",
       "      <th>val_categorical_accuracy</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "      <th>auc</th>\n",
       "      <th>val_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.514329</td>\n",
       "      <td>0.846451</td>\n",
       "      <td>0.861413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>(60, 60)</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.475113</td>\n",
       "      <td>0.820293</td>\n",
       "      <td>0.833387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>(60, 60)</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.432881</td>\n",
       "      <td>0.751547</td>\n",
       "      <td>0.828625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.453997</td>\n",
       "      <td>0.810648</td>\n",
       "      <td>0.825137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>(60, 60)</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.434389</td>\n",
       "      <td>0.802486</td>\n",
       "      <td>0.679350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>(60, 60)</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.703632</td>\n",
       "      <td>0.650863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>(60, 60)</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.226244</td>\n",
       "      <td>0.547674</td>\n",
       "      <td>0.599325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>(60, 60)</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.218703</td>\n",
       "      <td>0.532682</td>\n",
       "      <td>0.558313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>(60, 60)</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.208145</td>\n",
       "      <td>0.514927</td>\n",
       "      <td>0.547850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.506582</td>\n",
       "      <td>0.546937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>(60, 60)</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.194570</td>\n",
       "      <td>0.507360</td>\n",
       "      <td>0.544963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.203620</td>\n",
       "      <td>0.509402</td>\n",
       "      <td>0.538812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.197587</td>\n",
       "      <td>0.497401</td>\n",
       "      <td>0.524800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.200603</td>\n",
       "      <td>0.492509</td>\n",
       "      <td>0.519325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.193062</td>\n",
       "      <td>0.504629</td>\n",
       "      <td>0.519225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.191554</td>\n",
       "      <td>0.500710</td>\n",
       "      <td>0.516750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    after_flatten_layer_size  after_flatten_layer_jump  \\\n",
       "14                         3                         3   \n",
       "11                         3                         2   \n",
       "9                          3                         2   \n",
       "8                          3                         2   \n",
       "13                         3                         3   \n",
       "7                          2                         3   \n",
       "3                          2                         2   \n",
       "15                         3                         3   \n",
       "1                          2                         2   \n",
       "10                         3                         2   \n",
       "5                          2                         3   \n",
       "6                          2                         3   \n",
       "12                         3                         3   \n",
       "0                          2                         2   \n",
       "2                          2                         2   \n",
       "4                          2                         3   \n",
       "\n",
       "    image_frames_to_retain image_shape  val_categorical_accuracy  \\\n",
       "14                      20    (40, 40)                      0.53   \n",
       "11                      20    (60, 60)                      0.49   \n",
       "9                       16    (60, 60)                      0.53   \n",
       "8                       16    (40, 40)                      0.53   \n",
       "13                      16    (60, 60)                      0.24   \n",
       "7                       20    (60, 60)                      0.34   \n",
       "3                       20    (60, 60)                      0.31   \n",
       "15                      20    (60, 60)                      0.25   \n",
       "1                       16    (60, 60)                      0.23   \n",
       "10                      20    (40, 40)                      0.20   \n",
       "5                       16    (60, 60)                      0.22   \n",
       "6                       20    (40, 40)                      0.25   \n",
       "12                      16    (40, 40)                      0.21   \n",
       "0                       16    (40, 40)                      0.23   \n",
       "2                       20    (40, 40)                      0.23   \n",
       "4                       16    (40, 40)                      0.17   \n",
       "\n",
       "    categorical_accuracy       auc   val_auc  \n",
       "14              0.514329  0.846451  0.861413  \n",
       "11              0.475113  0.820293  0.833387  \n",
       "9               0.432881  0.751547  0.828625  \n",
       "8               0.453997  0.810648  0.825137  \n",
       "13              0.434389  0.802486  0.679350  \n",
       "7               0.358974  0.703632  0.650863  \n",
       "3               0.226244  0.547674  0.599325  \n",
       "15              0.218703  0.532682  0.558313  \n",
       "1               0.208145  0.514927  0.547850  \n",
       "10              0.196078  0.506582  0.546937  \n",
       "5               0.194570  0.507360  0.544963  \n",
       "6               0.203620  0.509402  0.538812  \n",
       "12              0.197587  0.497401  0.524800  \n",
       "0               0.200603  0.492509  0.519325  \n",
       "2               0.193062  0.504629  0.519225  \n",
       "4               0.191554  0.500710  0.516750  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for res in global_storage:\n",
    "    keys = list(res.keys())\n",
    "    search_auc_reg = re.compile('^auc_.*$')            \n",
    "    search_auc_key = list(filter(search_auc_reg.match, keys))\n",
    "    \n",
    "    search_auc_val_reg = re.compile('^.+auc_.*$')            \n",
    "    search_auc_val_key = list(filter(search_auc_val_reg.match, keys))\n",
    "    res['auc'] = res[search_auc_key[0]]\n",
    "    res['val_auc'] = res[search_auc_val_key[0]]\n",
    "    \n",
    "\n",
    "    \n",
    "df = pd.DataFrame.from_dict(global_storage).sort_values('val_auc', axis=0, ascending=False)\n",
    "\n",
    "\n",
    "df_all_layer = df.loc[:,['after_flatten_layer_size', 'after_flatten_layer_jump',\n",
    "                                        'image_frames_to_retain', 'image_shape', 'val_categorical_accuracy',\n",
    "                                       'categorical_accuracy', 'auc', 'val_auc']]\n",
    "\n",
    "display(df_all_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with after flatten layer size -> 3 , layer jump 3 , image frames to retain 20 and image shape (40, 40) outperforms other model with the availablle data . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_storage = []\n",
    "num_epochs = 11\n",
    "batch_size = 4\n",
    "test_model(conv3d_model, layers=[3, 4], \n",
    "           initial_filter_sizes=[30], \n",
    "           filter_jumps=[2], after_flatten_layer_sizes=[3],\n",
    "           after_flatten_layer_jumps=[3],\n",
    "           image_frames_to_retain=[20], image_shapes=[(40, 40)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>after_flatten_layer_size</th>\n",
       "      <th>after_flatten_layer_jump</th>\n",
       "      <th>image_frames_to_retain</th>\n",
       "      <th>image_shape</th>\n",
       "      <th>val_categorical_accuracy</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "      <th>auc</th>\n",
       "      <th>val_auc</th>\n",
       "      <th>layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.865762</td>\n",
       "      <td>0.984950</td>\n",
       "      <td>0.900450</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.831071</td>\n",
       "      <td>0.975622</td>\n",
       "      <td>0.814875</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   after_flatten_layer_size  after_flatten_layer_jump  image_frames_to_retain  \\\n",
       "1                         3                         3                      20   \n",
       "0                         3                         3                      20   \n",
       "\n",
       "  image_shape  val_categorical_accuracy  categorical_accuracy       auc  \\\n",
       "1    (40, 40)                      0.68              0.865762  0.984950   \n",
       "0    (40, 40)                      0.53              0.831071  0.975622   \n",
       "\n",
       "    val_auc  layer  \n",
       "1  0.900450      4  \n",
       "0  0.814875      3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for res in global_storage:\n",
    "    keys = list(res.keys())\n",
    "    search_auc_reg = re.compile('^auc_.*$')            \n",
    "    search_auc_key = list(filter(search_auc_reg.match, keys))\n",
    "    \n",
    "    search_auc_val_reg = re.compile('^.+auc_.*$')            \n",
    "    search_auc_val_key = list(filter(search_auc_val_reg.match, keys))\n",
    "    res['auc'] = res[search_auc_key[0]]\n",
    "    res['val_auc'] = res[search_auc_val_key[0]]\n",
    "    \n",
    "\n",
    "    \n",
    "df = pd.DataFrame.from_dict(global_storage).sort_values('val_auc', axis=0, ascending=False)\n",
    "\n",
    "\n",
    "df_all_layer = df.loc[:,['after_flatten_layer_size', 'after_flatten_layer_jump',\n",
    "                                        'image_frames_to_retain', 'image_shape', 'val_categorical_accuracy',\n",
    "                                       'categorical_accuracy', 'auc', 'val_auc', 'layer']]\n",
    "\n",
    "display(df_all_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models with layer 3 and 4 Conv3d perform eqaully well. Below is a working copy of the same ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model(conv3d_model, layers=[3, 4], \n",
    "#            initial_filter_sizes=[30], \n",
    "#            filter_jumps=[2], after_flatten_layer_sizes=[3],\n",
    "#            after_flatten_layer_jumps=[3],\n",
    "#            image_frames_to_retain=[20], image_shapes=[(40, 40)])\n",
    "model = Sequential()\n",
    "\n",
    "# layer group 1\n",
    "\n",
    "model.add(\n",
    "        Conv3D(\n",
    "        filters=30,\n",
    "        kernel_size=(2, 2, 2),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        input_shape=(20, 40, 40, 3)\n",
    "    )\n",
    ")\n",
    "        \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# layer group 2\n",
    "\n",
    "model.add(\n",
    "        Conv3D(\n",
    "        filters=60,\n",
    "        kernel_size=(2, 2, 2),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "        \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# layer group 3\n",
    "\n",
    "model.add(\n",
    "        Conv3D(\n",
    "        filters=120,\n",
    "        kernel_size=(2, 2, 2),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "        \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# layer group 4\n",
    "\n",
    "model.add(\n",
    "        Conv3D(\n",
    "        filters=240,\n",
    "        kernel_size=(2, 2, 2),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "        \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "\n",
    "# flatten\n",
    "\n",
    "model.add(\n",
    "    Flatten()\n",
    ")\n",
    "\n",
    "#final\n",
    "\n",
    "model.add(Dense(45, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_5 (Conv3D)            (None, 20, 40, 40, 30)    750       \n",
      "_________________________________________________________________\n",
      "batch_normalization_146 (Bat (None, 20, 40, 40, 30)    120       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 10, 20, 20, 30)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 10, 20, 20, 60)    14460     \n",
      "_________________________________________________________________\n",
      "batch_normalization_147 (Bat (None, 10, 20, 20, 60)    240       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_6 (MaxPooling3 (None, 5, 10, 10, 60)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 5, 10, 10, 120)    57720     \n",
      "_________________________________________________________________\n",
      "batch_normalization_148 (Bat (None, 5, 10, 10, 120)    480       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 2, 5, 5, 120)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 2, 5, 5, 240)      230640    \n",
      "_________________________________________________________________\n",
      "batch_normalization_149 (Bat (None, 2, 5, 5, 240)      960       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 1, 2, 2, 240)      0         \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 960)               0         \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 45)                43245     \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 15)                690       \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 349,385\n",
      "Trainable params: 348,485\n",
      "Non-trainable params: 900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.SGD()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy', AUC()])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation = -1\n",
    "train_generator = generator(train_path, train_doc, batch_size, \n",
    "                            ablation=ablation, image_frames_to_retain=20, \n",
    "                            image_shape=(40, 40))\n",
    "val_generator = generator(val_path, val_doc, batch_size, \n",
    "                            ablation=ablation, image_frames_to_retain=20, \n",
    "                            image_shape=(40, 40))\n",
    "num_epochs = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_final = []\n",
    "\n",
    "class ComputeMetrics(Callback):\n",
    "    \n",
    "    def __init__(self,  \n",
    "                 *args, **kwargs):\n",
    "        \n",
    "        self.storage = storage_final\n",
    "        \n",
    "        \n",
    "        super().__init__(*args, **kwargs);\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        temp_storage = {}\n",
    "        temp_storage['epoch'] = epoch\n",
    "        temp_storage.update(logs)\n",
    "        self.storage.append(temp_storage)\n",
    "\n",
    "\n",
    "model_name = 'model_init3' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, cooldown=1)\n",
    "callbacks_list = [checkpoint, LR, ComputeMetrics()]\n",
    "#callbacks_list = [ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 25\n"
     ]
    }
   ],
   "source": [
    "if (num_train_sequences%batch_size) == 0 and ablation == -1:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "elif ablation == -1:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "elif ((ablation * 5)%batch_size) == 0 and ablation != -1:\n",
    "    steps_per_epoch = int(ablation * 5/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = int(ablation * 5//batch_size)\n",
    "    \n",
    "\n",
    "if (num_val_sequences%batch_size) == 0 and ablation == -1:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "elif ablation == -1:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "elif ((ablation * 5)%batch_size) == 0 and ablation != -1:\n",
    "    validation_steps = int(ablation * 5/batch_size)    \n",
    "else:\n",
    "    validation_steps = int(ablation * 5//batch_size)\n",
    "\n",
    "print(steps_per_epoch, validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "166/166 [==============================] - 146s 880ms/step - loss: 1.6043 - categorical_accuracy: 0.2655 - auc_50: 0.5661 - val_loss: 1.6216 - val_categorical_accuracy: 0.2100 - val_auc_50: 0.4784\n",
      "\n",
      "Epoch 00001: saving model to model_init3_2020-12-2710_53_07.763774/model-00001-1.60426-0.26546-1.62160-0.21000.h5\n",
      "Epoch 2/11\n",
      "166/166 [==============================] - 143s 861ms/step - loss: 1.4006 - categorical_accuracy: 0.3424 - auc_50: 0.7340 - val_loss: 1.5992 - val_categorical_accuracy: 0.3300 - val_auc_50: 0.6902\n",
      "\n",
      "Epoch 00002: saving model to model_init3_2020-12-2710_53_07.763774/model-00002-1.40101-0.34238-1.59917-0.33000.h5\n",
      "Epoch 3/11\n",
      "166/166 [==============================] - 142s 855ms/step - loss: 1.2933 - categorical_accuracy: 0.3891 - auc_50: 0.7829 - val_loss: 1.5108 - val_categorical_accuracy: 0.4600 - val_auc_50: 0.7866\n",
      "\n",
      "Epoch 00003: saving model to model_init3_2020-12-2710_53_07.763774/model-00003-1.29304-0.38914-1.51079-0.46000.h5\n",
      "Epoch 4/11\n",
      "166/166 [==============================] - 142s 854ms/step - loss: 1.2886 - categorical_accuracy: 0.4027 - auc_50: 0.7793 - val_loss: 1.3949 - val_categorical_accuracy: 0.4700 - val_auc_50: 0.8166\n",
      "\n",
      "Epoch 00004: saving model to model_init3_2020-12-2710_53_07.763774/model-00004-1.28913-0.40271-1.39485-0.47000.h5\n",
      "Epoch 5/11\n",
      "166/166 [==============================] - 143s 859ms/step - loss: 1.2210 - categorical_accuracy: 0.4736 - auc_50: 0.8085 - val_loss: 0.6532 - val_categorical_accuracy: 0.5500 - val_auc_50: 0.8321\n",
      "\n",
      "Epoch 00005: saving model to model_init3_2020-12-2710_53_07.763774/model-00005-1.22069-0.47360-0.65321-0.55000.h5\n",
      "Epoch 6/11\n",
      "166/166 [==============================] - 142s 857ms/step - loss: 1.1419 - categorical_accuracy: 0.5158 - auc_50: 0.8353 - val_loss: 0.8177 - val_categorical_accuracy: 0.5600 - val_auc_50: 0.8436\n",
      "\n",
      "Epoch 00006: saving model to model_init3_2020-12-2710_53_07.763774/model-00006-1.14215-0.51584-0.81770-0.56000.h5\n",
      "Epoch 7/11\n",
      "166/166 [==============================] - 143s 859ms/step - loss: 1.0626 - categorical_accuracy: 0.5475 - auc_50: 0.8589 - val_loss: 1.3506 - val_categorical_accuracy: 0.5700 - val_auc_50: 0.8494\n",
      "\n",
      "Epoch 00007: saving model to model_init3_2020-12-2710_53_07.763774/model-00007-1.06243-0.54751-1.35056-0.57000.h5\n",
      "Epoch 8/11\n",
      "166/166 [==============================] - 143s 860ms/step - loss: 1.0225 - categorical_accuracy: 0.5641 - auc_50: 0.8686 - val_loss: 1.1802 - val_categorical_accuracy: 0.5600 - val_auc_50: 0.8723\n",
      "\n",
      "Epoch 00008: saving model to model_init3_2020-12-2710_53_07.763774/model-00008-1.02284-0.56410-1.18024-0.56000.h5\n",
      "Epoch 9/11\n",
      "166/166 [==============================] - 143s 859ms/step - loss: 0.9578 - categorical_accuracy: 0.6380 - auc_50: 0.8843 - val_loss: 0.4753 - val_categorical_accuracy: 0.6000 - val_auc_50: 0.8659\n",
      "\n",
      "Epoch 00009: saving model to model_init3_2020-12-2710_53_07.763774/model-00009-0.95733-0.63801-0.47530-0.60000.h5\n",
      "Epoch 10/11\n",
      "166/166 [==============================] - 143s 860ms/step - loss: 0.8227 - categorical_accuracy: 0.6938 - auc_50: 0.9180 - val_loss: 1.3204 - val_categorical_accuracy: 0.6000 - val_auc_50: 0.8742\n",
      "\n",
      "Epoch 00010: saving model to model_init3_2020-12-2710_53_07.763774/model-00010-0.82312-0.69382-1.32038-0.60000.h5\n",
      "Epoch 11/11\n",
      "166/166 [==============================] - 143s 859ms/step - loss: 0.7870 - categorical_accuracy: 0.7134 - auc_50: 0.9206 - val_loss: 1.7663 - val_categorical_accuracy: 0.5700 - val_auc_50: 0.8203\n",
      "\n",
      "Epoch 00011: saving model to model_init3_2020-12-2710_53_07.763774/model-00011-0.78678-0.71342-1.76630-0.57000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f519741af60>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0, \n",
    "                    use_multiprocessing=False)\n",
    "\n",
    "# model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "#                      class_weight=None, workers=1, initial_epoch=0, \n",
    "#                     use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_categorical_accuracy</th>\n",
       "      <th>val_auc_50</th>\n",
       "      <th>loss</th>\n",
       "      <th>categorical_accuracy</th>\n",
       "      <th>auc_50</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.475303</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.865887</td>\n",
       "      <td>0.957325</td>\n",
       "      <td>0.638009</td>\n",
       "      <td>0.884348</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.320385</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.874225</td>\n",
       "      <td>0.823125</td>\n",
       "      <td>0.693816</td>\n",
       "      <td>0.918041</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.350562</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.849450</td>\n",
       "      <td>1.062428</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.858889</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1.766303</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.820287</td>\n",
       "      <td>0.786782</td>\n",
       "      <td>0.713424</td>\n",
       "      <td>0.920642</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.817698</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.843587</td>\n",
       "      <td>1.142147</td>\n",
       "      <td>0.515837</td>\n",
       "      <td>0.835349</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.180238</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.872337</td>\n",
       "      <td>1.022840</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.868555</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.653211</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.832100</td>\n",
       "      <td>1.220686</td>\n",
       "      <td>0.473605</td>\n",
       "      <td>0.808522</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.394853</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.816550</td>\n",
       "      <td>1.289126</td>\n",
       "      <td>0.402715</td>\n",
       "      <td>0.779315</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.510787</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.786600</td>\n",
       "      <td>1.293037</td>\n",
       "      <td>0.389140</td>\n",
       "      <td>0.782854</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.599175</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>1.401011</td>\n",
       "      <td>0.342383</td>\n",
       "      <td>0.733985</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.621604</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.478437</td>\n",
       "      <td>1.604263</td>\n",
       "      <td>0.265460</td>\n",
       "      <td>0.566113</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  val_loss  val_categorical_accuracy  val_auc_50      loss  \\\n",
       "8       8  0.475303                      0.60    0.865887  0.957325   \n",
       "9       9  1.320385                      0.60    0.874225  0.823125   \n",
       "6       6  1.350562                      0.57    0.849450  1.062428   \n",
       "10     10  1.766303                      0.57    0.820287  0.786782   \n",
       "5       5  0.817698                      0.56    0.843587  1.142147   \n",
       "7       7  1.180238                      0.56    0.872337  1.022840   \n",
       "4       4  0.653211                      0.55    0.832100  1.220686   \n",
       "3       3  1.394853                      0.47    0.816550  1.289126   \n",
       "2       2  1.510787                      0.46    0.786600  1.293037   \n",
       "1       1  1.599175                      0.33    0.690200  1.401011   \n",
       "0       0  1.621604                      0.21    0.478437  1.604263   \n",
       "\n",
       "    categorical_accuracy    auc_50    lr  \n",
       "8               0.638009  0.884348  0.01  \n",
       "9               0.693816  0.918041  0.01  \n",
       "6               0.547511  0.858889  0.01  \n",
       "10              0.713424  0.920642  0.01  \n",
       "5               0.515837  0.835349  0.01  \n",
       "7               0.564103  0.868555  0.01  \n",
       "4               0.473605  0.808522  0.01  \n",
       "3               0.402715  0.779315  0.01  \n",
       "2               0.389140  0.782854  0.01  \n",
       "1               0.342383  0.733985  0.01  \n",
       "0               0.265460  0.566113  0.01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "fig, ax =plt.subplots(1,2)\n",
    "\n",
    "df = pd.DataFrame.from_dict(storage_final)\n",
    "\n",
    "search_auc_val_reg = re.compile('^.+auc_.*$')            \n",
    "search_auc_val_key = list(filter(search_auc_val_reg.match, storage_final[0].keys()))\n",
    "\n",
    "search_auc_reg = re.compile('^auc_.*$')            \n",
    "search_auc_key = list(filter(search_auc_reg.match, storage_final[0].keys()))\n",
    "\n",
    "categoical_analysis = df.loc[:,['val_categorical_accuracy', \n",
    "                                'categorical_accuracy', 'epoch']].melt('epoch', var_name='cols', value_name='vals')\n",
    "\n",
    "val_analysis = df.loc[:,[search_auc_val_key[0], \n",
    "                              search_auc_key[0]  , 'epoch']].melt('epoch', var_name='cols', value_name='vals')\n",
    "\n",
    "loss_analysis = df.loc[:,['loss', 'val_loss','epoch']].melt('epoch', var_name='cols', value_name='vals')\n",
    "\n",
    "\n",
    "sns.lineplot(data=categoical_analysis, x='epoch', y='vals', hue='cols', ax=ax[0])\n",
    "sns.lineplot(data=val_analysis, x='epoch', y='vals', hue='cols', ax=ax[1])\n",
    "# sns.lineplot(data=loss_analysis, x='epoch', y='vals', hue='cols', ax=ax[1, 0])\n",
    "\n",
    "display(df.sort_values('val_categorical_accuracy', axis=0, ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence , as shown above , epoch 6 performs best with validation auc 0.91 and and validation val_categorical_accuracy 0.73. Running it multiple times has also provided result with 0.76 accuracy . The total trainable params are  348,485 . We could have added more layers and dense networks for better accuracy , but the number of parameters would have increases which would have contradicted our requirement of keeping it minimum. \n",
    "\n",
    "Decreasing layers any further (or decreasing the params leads to acc going below 0.60)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
